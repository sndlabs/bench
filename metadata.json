{
  "last_updated": "2025-07-29T16:09:09.864972",
  "total_runs": 22,
  "models": {
    "qwen2.5-1.5b-instruct-q4_k_m.gguf": {
      "runs": 9,
      "best_accuracy": 0,
      "latest_run": "20250729_004631"
    },
    "meta-llama/Llama-2-7b-hf": {
      "runs": 1,
      "best_accuracy": 0,
      "latest_run": "lm-eval_20250729_101249"
    },
    "gpt2": {
      "runs": 11,
      "best_accuracy": 0.47013403263403264,
      "latest_run": "lm-eval_20250729_154839"
    },
    "mistralai/Mistral-7B-v0.1": {
      "runs": 1,
      "best_accuracy": 0,
      "latest_run": "lm-eval_20250729_103028"
    }
  },
  "runs": [
    {
      "run_id": "20250728_154516",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-28T15:45:18.753833",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250728_224700",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-28T22:47:03.516361",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250728_232219",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-28T23:22:21.941064",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250728_233812",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-28T23:38:14.369948",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250729_000923",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-29T00:09:25.813340",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250729_002147",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-29T00:21:49.394243",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250729_002327",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-29T00:23:30.236864",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250729_002828",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-29T00:28:30.645059",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "20250729_004631",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "timestamp": "2025-07-29T00:46:34.502497",
      "average_accuracy": 0.0,
      "tasks": [
        "perplexity"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_101249",
      "model": "meta-llama/Llama-2-7b-hf",
      "timestamp": "2025-07-29T10:12:50.118378",
      "average_accuracy": 0.0,
      "tasks": [],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_102437",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:26:17.612973",
      "average_accuracy": 0.0,
      "tasks": [],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_102746",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:29:25.124766",
      "average_accuracy": 0.0,
      "tasks": [],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_103028",
      "model": "mistralai/Mistral-7B-v0.1",
      "timestamp": "2025-07-29T10:30:29.277352",
      "average_accuracy": 0.0,
      "tasks": [],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_103331",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:35:07.419630",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_103534",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:37:13.405518",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_103822",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:40:00.627731",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_104952",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:51:31.269939",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_105733",
      "model": "gpt2",
      "timestamp": "2025-07-29T10:59:10.692027",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_110714",
      "model": "gpt2",
      "timestamp": "2025-07-29T11:08:49.577854",
      "average_accuracy": 0.43813131313131315,
      "tasks": [
        "arc_easy"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_111954",
      "model": "gpt2",
      "timestamp": "2025-07-29T11:23:38.023444",
      "average_accuracy": 0.47013403263403264,
      "tasks": [
        "arc_easy",
        "kobest_boolq"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_122602",
      "model": "gpt2",
      "timestamp": "2025-07-29T12:29:38.749071",
      "average_accuracy": 0.47013403263403264,
      "tasks": [
        "arc_easy",
        "kobest_boolq"
      ],
      "wandb_url": null,
      "wandb_id": null
    },
    {
      "run_id": "lm-eval_20250729_154839",
      "model": "gpt2",
      "timestamp": "2025-07-29T16:09:09.116564",
      "average_accuracy": 0.18540484367346627,
      "tasks": [
        "kmmlu",
        "kmmlu_applied_science",
        "kmmlu_aviation_engineering_and_maintenance",
        "kmmlu_electronics_engineering",
        "kmmlu_energy_management",
        "kmmlu_environmental_science",
        "kmmlu_gas_technology_and_engineering",
        "kmmlu_geomatics",
        "kmmlu_industrial_engineer",
        "kmmlu_machine_design_and_manufacturing",
        "kmmlu_maritime_engineering",
        "kmmlu_nondestructive_testing",
        "kmmlu_railway_and_automotive_engineering",
        "kmmlu_telecommunications_and_wireless_technology",
        "kmmlu_humss",
        "kmmlu_accounting",
        "kmmlu_criminal_law",
        "kmmlu_economics",
        "kmmlu_education",
        "kmmlu_korean_history",
        "kmmlu_law",
        "kmmlu_management",
        "kmmlu_political_science_and_sociology",
        "kmmlu_psychology",
        "kmmlu_social_welfare",
        "kmmlu_taxation",
        "kmmlu_other",
        "kmmlu_agricultural_sciences",
        "kmmlu_construction",
        "kmmlu_fashion",
        "kmmlu_food_processing",
        "kmmlu_health",
        "kmmlu_interior_architecture_and_design",
        "kmmlu_marketing",
        "kmmlu_patent",
        "kmmlu_public_safety",
        "kmmlu_real_estate",
        "kmmlu_refrigerating_machinery",
        "kmmlu_stem",
        "kmmlu_biology",
        "kmmlu_chemical_engineering",
        "kmmlu_chemistry",
        "kmmlu_civil_engineering",
        "kmmlu_computer_science",
        "kmmlu_ecology",
        "kmmlu_electrical_engineering",
        "kmmlu_information_technology",
        "kmmlu_materials_engineering",
        "kmmlu_math",
        "kmmlu_mechanical_engineering",
        "mmlu",
        "mmlu_humanities",
        "mmlu_formal_logic",
        "mmlu_high_school_european_history",
        "mmlu_high_school_us_history",
        "mmlu_high_school_world_history",
        "mmlu_international_law",
        "mmlu_jurisprudence",
        "mmlu_logical_fallacies",
        "mmlu_moral_disputes",
        "mmlu_moral_scenarios",
        "mmlu_philosophy",
        "mmlu_prehistory",
        "mmlu_professional_law",
        "mmlu_world_religions",
        "mmlu_other",
        "mmlu_business_ethics",
        "mmlu_clinical_knowledge",
        "mmlu_college_medicine",
        "mmlu_global_facts",
        "mmlu_human_aging",
        "mmlu_management",
        "mmlu_marketing",
        "mmlu_medical_genetics",
        "mmlu_miscellaneous",
        "mmlu_nutrition",
        "mmlu_professional_accounting",
        "mmlu_professional_medicine",
        "mmlu_virology",
        "mmlu_social_sciences",
        "mmlu_econometrics",
        "mmlu_high_school_geography",
        "mmlu_high_school_government_and_politics",
        "mmlu_high_school_macroeconomics",
        "mmlu_high_school_microeconomics",
        "mmlu_high_school_psychology",
        "mmlu_human_sexuality",
        "mmlu_professional_psychology",
        "mmlu_public_relations",
        "mmlu_security_studies",
        "mmlu_sociology",
        "mmlu_us_foreign_policy",
        "mmlu_stem",
        "mmlu_abstract_algebra",
        "mmlu_anatomy",
        "mmlu_astronomy",
        "mmlu_college_biology",
        "mmlu_college_chemistry",
        "mmlu_college_computer_science",
        "mmlu_college_mathematics",
        "mmlu_college_physics",
        "mmlu_computer_security",
        "mmlu_conceptual_physics",
        "mmlu_electrical_engineering",
        "mmlu_elementary_mathematics",
        "mmlu_high_school_biology",
        "mmlu_high_school_chemistry",
        "mmlu_high_school_computer_science",
        "mmlu_high_school_mathematics",
        "mmlu_high_school_physics",
        "mmlu_high_school_statistics",
        "mmlu_machine_learning"
      ],
      "wandb_url": null,
      "wandb_id": null
    }
  ]
}