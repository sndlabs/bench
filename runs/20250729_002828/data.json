{
  "run_id": "20250729_002828",
  "timestamp": "2025-07-29T00:28:30.645059",
  "model": {
    "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
    "path": "/Users/victor/GitHub/snd-bench/models/qwen2.5-1.5b-instruct-q4_k_m.gguf",
    "size": "1.0G"
  },
  "results": {
    "perplexity": {
      "accuracy": 0,
      "stderr": 0,
      "samples": 0
    }
  },
  "average_accuracy": 0.0,
  "total_tasks": 1,
  "wandb_history": [
    {
      "id": "v50s2235",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250729_002828",
      "created_at": "2025-07-28T15:28:32Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/v50s2235",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "ztujkzvz",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250729_002327",
      "created_at": "2025-07-28T15:23:31Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/ztujkzvz",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "ynlgwd36",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250729_002147",
      "created_at": "2025-07-28T15:21:50Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/ynlgwd36",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "78qicwqu",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250729_000923",
      "created_at": "2025-07-28T15:09:27Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/78qicwqu",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "43zib5if",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250728_233812",
      "created_at": "2025-07-28T14:38:16Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/43zib5if",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "67tcxo0t",
      "name": "qwen2.5-1.5b-instruct-q4_k_m.gguf-20250728_232219",
      "created_at": "2025-07-28T14:22:24Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/67tcxo0t",
      "model": "qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "metrics": {
        "average_accuracy": 0,
        "model_size_gb": 1,
        "task_perplexity": 0
      }
    },
    {
      "id": "gtl3262m",
      "name": "test-model-q4_k_m.gguf-test_wandb_20250728",
      "created_at": "2025-07-28T14:10:08Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/gtl3262m",
      "model": "test-model-q4_k_m.gguf",
      "metrics": {
        "perplexity": 12.5,
        "model_size_gb": 1.5
      }
    },
    {
      "id": "m3x28hiy",
      "name": "Pipeline Test 2025-07-22 22:27",
      "created_at": "2025-07-22T13:27:23Z",
      "url": "https://wandb.ai/sndlabs/llm-bench/runs/m3x28hiy",
      "model": "qwen2.5-1.5b-instruct-q4_k_m",
      "metrics": {}
    }
  ],
  "summary": "## Executive Summary: Qwen 2.5 1.5B Benchmark Analysis\n\nThe benchmark run for the Qwen 2.5 1.5B instruction-tuned model (quantized to 4-bit) reveals a critical failure in the evaluation process. The model achieved 0% accuracy across all metrics, with zero samples successfully processed during the perplexity evaluation. This indicates a fundamental issue with either the benchmark execution, model compatibility, or the evaluation framework itself, rather than poor model performance.\n\nThe lightweight 1.0GB model size suggests aggressive quantization (Q4_K_M format), which typically trades some accuracy for significant memory savings. However, the complete absence of successful evaluations points to a technical failure rather than quantization-related performance degradation. The perplexity task, which measures how well the model predicts text, failed to process any samples, suggesting potential issues with input formatting, tokenization, or model loading.\n\nThis benchmark run has been tracked in Weights & Biases ([Run v50s2235](https://wandb.ai/sndlabs/llm-bench/runs/v50s2235)) for detailed analysis. Immediate investigation is recommended to identify whether this is a configuration issue, compatibility problem with the GGUF format, or a bug in the benchmark harness. Re-running the benchmark with verbose logging and verifying the model loads correctly should be the next steps before drawing any conclusions about the model's actual capabilities."
}
